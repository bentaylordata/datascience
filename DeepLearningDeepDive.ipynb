{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greatest Python Deep Learning Tutorial EVER...\n",
    "Theano introduction example with MNIST, babysteps, simple to understand, easy to follow. No tricks. Other python deep learning tutorial suffer from *\"death by wrappers\"* where they wrap so many libraries you begin to wonder what you are actually driving. Also, deep learning is a simple enough concept you should be able to comprehend it from the ground up. This tutorial was built by @bentaylordata\n",
    "![caption](http://7-themes.com/data_images/out/37/6896773-amazing-wallpaper.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Grab Some Data! Hello World Deep Learning Example ~ MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "dataset = datasets.fetch_mldata(\"MNIST Original\")\n",
    "X = dataset.data\n",
    "Y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we have 70000 hand written digit images. The images have been flattened where 28x28 = 784\n",
      "The labels are a vector  70000 long. With unique values: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n"
     ]
    }
   ],
   "source": [
    "print \"So we have\",X.shape[0],\"hand written digit images. The images have been flattened where 28x28 =\",X.shape[1]\n",
    "print \"The labels are a vector \",len(Y),\"long. With unique values:\",np.unique(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Of A Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have reshaped (70000, 784) into (70000, 1, 28, 28) . This is the format we want for convolutional nets\n"
     ]
    }
   ],
   "source": [
    "X_tensor = X.reshape((X.shape[0],1,28,28))\n",
    "print \"We have reshaped\",X.shape,\"into\",X_tensor.shape,\". This is the format we want for convolutional nets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Create Smart Train/Test Splits for training where we can train on **86%** of the data and test on **14%** using a 7 k-fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "stratified_k_fold_splits = StratifiedKFold(Y, n_folds=7,shuffle=True)\n",
    "for train_index, test_index in stratified_k_fold_splits: break  #Just take for the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(train_index)     # Index shuffle not working? Force here\n",
    "np.random.shuffle(test_index)      # Index shuffle not working? Force here\n",
    "X_train, Y_train = X_tensor[train_index], Y[train_index]\n",
    "X_test, Y_test = X_tensor[test_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Create A Second Training Validation Set To Use For An Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stratified_k_fold_splits = StratifiedKFold(Y_train, n_folds=7,shuffle=True)\n",
    "for train_train_index, train_test_index in stratified_k_fold_splits: break  #Just take for the first one\n",
    "X_train_train, Y_train_train = X_tensor[train_index][train_train_index], Y[train_index][train_train_index]\n",
    "X_train_test, Y_train_test = X_tensor[train_index][train_test_index], Y[train_index][train_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set is: (59996, 1, 28, 28)\n",
      "     Our training (train) set is: (51422, 1, 28, 28)\n",
      "     Our training (test) set is: (8574, 1, 28, 28)\n",
      "Our testing set is: (10004, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print \"Our training set is:\",X_train.shape\n",
    "print \"     Our training (train) set is:\",X_train_train.shape\n",
    "print \"     Our training (test) set is:\",X_train_test.shape\n",
    "print \"Our testing set is:\",X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, nb_classes=None):\n",
    "    '''Convert class vector (integers from 0 to nb_classes)\n",
    "    to binary class matrix, for use with categorical_crossentropy\n",
    "    '''\n",
    "    y = np.asarray(y, dtype='int32')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes))\n",
    "    for i in range(len(y)):\n",
    "        Y[i, y[i]] = 1.\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59996, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_train = to_categorical(Y_train)                 # Convert Ys to matrix onehotencoded format for training\n",
    "Y_test = to_categorical(Y_test)\n",
    "Y_train_train = to_categorical(Y_train_train)\n",
    "Y_train_test = to_categorical(Y_train_test)\n",
    "print Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Preview, what are we actually doing?\n",
    "When dealing with image data it is always nice to preview it to see if you have messed up on the reshape, tranpose, etc... For the MNIST hand written dataset lets take a look at our numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "for number in range(0,10):\n",
    "    if number==0:\n",
    "        image_stack = X_train[np.argmax(Y_train,axis=1)==number][0][0]\n",
    "    else:\n",
    "        image_stack = np.hstack((image_stack,X_train[np.argmax(Y_train,axis=1)==number][0][0]))\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,10)\n",
    "plt.imshow(image_stack,cmap=plt.get_cmap('gray_r'))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59996, 1, 28, 28)\n",
      "(59996, 10)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Building Blocks\n",
    "Here are some simple building blocks for deep learning to help our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.downsample import max_pool_2d\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "srng = RandomStreams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def rectify(X):\n",
    "    return T.maximum(X, 0.)\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def model(X, w, w2, w3, w4, p_drop_conv, p_drop_hidden):\n",
    "    l1a = rectify(conv2d(X, w, border_mode='full'))\n",
    "    l1 = max_pool_2d(l1a, (2, 2))\n",
    "    l1 = dropout(l1, p_drop_conv)\n",
    "\n",
    "    l2a = rectify(conv2d(l1, w2))\n",
    "    l2 = max_pool_2d(l2a, (2, 2))\n",
    "    l2 = dropout(l2, p_drop_conv)\n",
    "\n",
    "    l3a = rectify(conv2d(l2, w3))\n",
    "    l3b = max_pool_2d(l3a, (2, 2))\n",
    "    l3 = T.flatten(l3b, outdim=2)\n",
    "    l3 = dropout(l3, p_drop_conv)\n",
    "\n",
    "    l4 = rectify(T.dot(l3, w4))\n",
    "    l4 = dropout(l4, p_drop_hidden)\n",
    "\n",
    "    pyx = softmax(T.dot(l4, w_o))\n",
    "    return l1, l2, l3, l4, pyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(cost, params, lr=0.05, momentum = 0.9):         # Stochastic gradient decent\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new =  acc*momentum + (1.0-momentum)*g\n",
    "        updates.append([acc, acc_new])\n",
    "        updates.append([p, p - acc_new * lr])\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define / Build A Deep Learning Model\n",
    "Now we will design a deep learning model for the MNIST data set that uses convolutions, dropout, and maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, w, p_drop_conv, p_drop_hidden):\n",
    "    layer1 = rectify(conv2d(X, w[0], border_mode='full'))\n",
    "    layer1 = max_pool_2d(layer1, (2, 2),ignore_border=True)\n",
    "    layer1 = dropout(layer1, p_drop_conv)\n",
    "\n",
    "    layer2 = rectify(conv2d(layer1, w[1]))\n",
    "    layer2 = max_pool_2d(layer2, (2, 2),ignore_border=True)\n",
    "    layer2 = dropout(layer2, p_drop_conv)\n",
    "\n",
    "    layer3 = rectify(conv2d(layer2, w[2]))\n",
    "    layer3 = max_pool_2d(layer3, (2, 2),ignore_border=True)\n",
    "    layer3 = T.flatten(layer3, outdim=2)\n",
    "    layer3 = dropout(layer3, p_drop_conv)\n",
    "\n",
    "    layer4 = rectify(T.dot(layer3, w[3]))\n",
    "    layer4 = dropout(layer4, p_drop_hidden)\n",
    "\n",
    "    pyx = softmax(T.dot(layer4, w_o))\n",
    "    return pyx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Backend Theano Functions To Initialize And Do The Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX=floatX(X_train)/255.0\n",
    "trY=floatX(Y_train)\n",
    "teX=floatX(X_test)/255.0\n",
    "teY=floatX(Y_test)\n",
    "X = T.ftensor4()\n",
    "Y = T.fmatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = init_weights((32, 1, 3, 3))\n",
    "w2 = init_weights((64, 32, 3, 3))\n",
    "w3 = init_weights((128, 64, 3, 3))\n",
    "w4 = init_weights((512, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "noise_py_x = model(X, [w, w2, w3, w4], 0.2, 0.5)\n",
    "py_x = model(X, [w, w2, w3, w4], 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "params = [w, w2, w3, w4, w_o]\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "updates = sgd(cost, params, lr=0.1)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 2.3025867939  Accuracy: 15.6037584966 percent\n",
      "Epoch: 1 Cost: 2.30258512497  Accuracy: 23.5805677729 percent\n",
      "Epoch: 2 Cost: 2.30257701874  Accuracy: 32.2570971611 percent\n",
      "Epoch: 3 Cost: 2.30256772041  Accuracy: 36.6153538585 percent\n",
      "Epoch: 4 Cost: 2.30256676674  Accuracy: 37.6849260296 percent\n",
      "Epoch: 5 Cost: 2.30255270004  Accuracy: 36.5853658537 percent\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(6):\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n",
    "        epoch_cost = train(trX[start:end], trY[start:end])\n",
    "    print \"Epoch:\",epoch,\"Cost:\",epoch_cost,\" Accuracy:\",np.mean(np.argmax(teY, axis=1) == predict(teX))*100,\"percent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = init_weights((32, 1, 3, 3))\n",
    "w2 = init_weights((64, 32, 3, 3))\n",
    "w3 = init_weights((128, 64, 3, 3))\n",
    "w4 = init_weights((128 * 4, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "updates = RMSprop(cost, params, lr=0.001)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 0.182546973228  Accuracy: 95.7716913235 percent\n",
      "Epoch: 1 Cost: 0.127789452672  Accuracy: 96.7113154738 percent\n",
      "Epoch: 2 Cost: 0.0231523327529  Accuracy: 98.2506997201 percent\n",
      "Epoch: 3 Cost: 0.0402584709227  Accuracy: 98.6605357857 percent\n",
      "Epoch: 4 Cost: 0.00932366959751  Accuracy: 98.7804878049 percent\n",
      "Epoch: 5 Cost: 0.0168595518917  Accuracy: 98.7604958017 percent\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(6):\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n",
    "        epoch_cost = train(trX[start:end], trY[start:end])\n",
    "    print \"Epoch:\",epoch,\"Cost:\",epoch_cost,\" Accuracy:\",np.mean(np.argmax(teY, axis=1) == predict(teX))*100,\"percent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok.... Now Lets Try A Theano Wrapper\n",
    "![caption](ss1.png)\n",
    "The great thing about Theano is you can understand EVERYTHING. There are some simple wrappers above Theano that make\n",
    "this process easier and offer more tools. My favorite is Keras.io. Check it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.insert(0, \"/home/ubuntu/keras\")\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_model(optimizer_algo):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, 1, 3, 3,border_mode='full'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 1, 3, 3,border_mode='full'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 1, 3, 3,border_mode='full'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(64, 32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(128, 64, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128*4,625))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(625,10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer_algo)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59996 samples, validate on 10004 samples\n",
      "Epoch 1/6\n",
      "59996/59996 [==============================] - 11s - loss: 0.3204 - acc: 0.8979 - val_loss: 0.0819 - val_acc: 0.9745\n",
      "Epoch 2/6\n",
      "59996/59996 [==============================] - 11s - loss: 0.0796 - acc: 0.9749 - val_loss: 0.0521 - val_acc: 0.9839\n",
      "Epoch 3/6\n",
      "59996/59996 [==============================] - 11s - loss: 0.0531 - acc: 0.9830 - val_loss: 0.0431 - val_acc: 0.9864\n",
      "Epoch 4/6\n",
      "59996/59996 [==============================] - 11s - loss: 0.0446 - acc: 0.9861 - val_loss: 0.0320 - val_acc: 0.9905\n",
      "Epoch 5/6\n",
      "59996/59996 [==============================] - 11s - loss: 0.0371 - acc: 0.9883 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 6/6\n",
      "59996/59996 [==============================] - 11s - loss: 0.0307 - acc: 0.9902 - val_loss: 0.0346 - val_acc: 0.9895\n"
     ]
    }
   ],
   "source": [
    "model = define_model('RMSprop')\n",
    "log_fit1 = model.fit(trX, trY, \n",
    "                    batch_size=128, \n",
    "                    nb_epoch=6, \n",
    "                    show_accuracy=True, \n",
    "                    verbose=1, \n",
    "                    validation_data=(teX, teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = define_model('adam')\n",
    "log_fit2 = model.fit(trX, trY, \n",
    "                    batch_size=128, \n",
    "                    nb_epoch=6, \n",
    "                    show_accuracy=True, \n",
    "                    verbose=1, \n",
    "                    validation_data=(teX, teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = define_model('sgd')\n",
    "log_fit3 = model.fit(trX, trY, \n",
    "                    batch_size=128, \n",
    "                    nb_epoch=6, \n",
    "                    show_accuracy=True, \n",
    "                    verbose=1, \n",
    "                    validation_data=(teX, teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(log_fit1.history['val_acc'],'o-',label='RMSprop')\n",
    "plt.plot(log_fit2.history['val_acc'],'o-',label='Adam')\n",
    "plt.plot(log_fit3.history['val_acc'],'o-',label='SGD')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Data augmentation can really help make your models more robust. This helps boost accuracy in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import generic_utils\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_color_model(optimizer_algo):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3, 3,border_mode='full'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(64, 32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(128, 64, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128*4,625))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(625,10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer_algo)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = define_color_model('adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig=plt.gcf()\n",
    "fig.set_size_inches(3,1)\n",
    "im1=np.array(np.transpose(x_train[0],axes=[1,2,0]))\n",
    "im2=np.array(np.transpose(x_train[1],axes=[1,2,0]))\n",
    "im3=np.array(np.transpose(x_train[2],axes=[1,2,0]))\n",
    "plt.imshow(np.hstack((im1,im2,im3)))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in range(6):\n",
    "    print('-'*40)\n",
    "    print('Epoch', e)\n",
    "    print('-'*40)\n",
    "    print(\"Training...\")\n",
    "    # batch train with realtime data augmentation\n",
    "    progbar = generic_utils.Progbar(X_train.shape[0])\n",
    "    for X_batch, Y_batch in datagen.flow(X_train, Y_train):\n",
    "        loss = model.train_on_batch(X_batch, Y_batch)\n",
    "        progbar.add(X_batch.shape[0], values=[(\"train loss\", loss)])\n",
    "\n",
    "    print(\"Testing...\")\n",
    "    # test time!\n",
    "    progbar = generic_utils.Progbar(X_test.shape[0])\n",
    "    for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n",
    "        score = model.test_on_batch(X_batch, Y_batch)\n",
    "        progbar.add(X_batch.shape[0], values=[(\"test loss\", score)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevent GPU starving...\n",
    "Use your CPUs for preprocessing data augmentation batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('buffering.py'):\n",
    "    print('Downloading buffering.py')\n",
    "    os.system('wget https://raw.githubusercontent.com/benanne/kaggle-ndsb/11a66cdbddee16c69514b9530a727df0ac6e136f/buffering.py')\n",
    "from buffering import buffered_gen_mp, buffered_gen_threaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in range(6):\n",
    "    print('-'*40)\n",
    "    print('Epoch', e)\n",
    "    print('-'*40)\n",
    "    print(\"Training...\")\n",
    "    # batch train with realtime data augmentation\n",
    "    progbar = generic_utils.Progbar(X_train.shape[0])\n",
    "    for X_batch, Y_batch in buffered_gen_threaded(datagen.flow(X_train, Y_train),buffer_size=8):\n",
    "        loss, acc = model.train_on_batch(X_batch, Y_batch, accuracy=True)\n",
    "        progbar.add(X_batch.shape[0], values=[(\"train loss\", loss),(\"train acc\", acc)])\n",
    "\n",
    "    print(\"Testing...\")\n",
    "    # test time!\n",
    "    progbar = generic_utils.Progbar(X_test.shape[0])\n",
    "    #p = np.array(np.zeros_like(Yp),dtype=float)\n",
    "    for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n",
    "        #p = model.predict_on_batch(X_batch)\n",
    "        score, acc = model.test_on_batch(X_batch, Y_batch, accuracy=True)\n",
    "        progbar.add(X_batch.shape[0], values=[(\"test loss\", score),(\"test acc\", acc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More Advanced Activations, leaky, PrLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More Advanced Max Pooling, Fractional Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More Advanced Data Augementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lessons Gotchas:\n",
    "    # Want sorted types in batches\n",
    "    # Weight initialization strategies\n",
    "    # Optimizations, key to success\n",
    "    # Don't Get Fooled By Imbalance Accuracies\n",
    "    # Don't CHEAT!! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
